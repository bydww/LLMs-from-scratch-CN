{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a58ca5",
   "metadata": {},
   "source": [
    "# 第五章 在无标签数据集上预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c882f",
   "metadata": {},
   "source": [
    "## 5.1 评估文本生成大模型\n",
    "### 5.1.1 用GPT来生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1254b182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import *\n",
    "import torch.nn.functional as F\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference\n",
    "#导入模型, 设定一系列参数, 设定随机种子确保可复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1a6d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output text:\n",
      " every effort moves you rentingetic minion mobilized Macicone heterogeneity\u0000achaRAM\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # Add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    token_ids = token_ids.squeeze(0)  # Remove batch dimension\n",
    "    decoded = tokenizer.decode(token_ids.tolist())\n",
    "    return decoded\n",
    "\n",
    "\n",
    "start_context = 'every effort moves you'\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# 举例说明\n",
    "\n",
    "token_ids = generate_text_simple(model=model,idx=text_to_token_ids(start_context, tokenizer), max_new_tokens=10, context_size=GPT_CONFIG_124M['context_length'])\n",
    "\n",
    "print(\"output text:\\n\",token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa491ae5",
   "metadata": {},
   "source": [
    "### 5.1.2&3 计算训练集和验证集的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd20e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 20479\n",
      "Total tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "file_path = 'the-verdict.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text_data = f.read()\n",
    "# 检查数据集中的字符数和词元数\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Total characters:\", total_characters)\n",
    "print(\"Total tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8f884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(total_characters * train_ratio)\n",
    "train_data = text_data[:split_idx]\n",
    "test_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab7f71",
   "metadata": {},
   "source": [
    "### 建立数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16868153",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(train_data,2,max_length=GPT_CONFIG_124M['context_length'],\n",
    "                                    stride=GPT_CONFIG_124M['context_length'],\n",
    "                                    shuffle=True,\n",
    "                                    drop_last=True,\n",
    "                                    num_workers=2)\n",
    "test_loader = create_dataloader_v1(test_data,2,max_length=GPT_CONFIG_124M['context_length'],\n",
    "                                    stride=GPT_CONFIG_124M['context_length'],\n",
    "                                    shuffle=True,\n",
    "                                    drop_last=True,\n",
    "                                    num_workers=2)\n",
    "                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "765916ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Test loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# 测试数据加载器\n",
    "print(\"Train loader:\")\n",
    "for x , y in train_loader:\n",
    "    print(x.shape,y.shape)\n",
    "\n",
    "\n",
    "print(\"\\nTest loader:\")\n",
    "for x , y in test_loader:\n",
    "    print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294823b",
   "metadata": {},
   "source": [
    "#### 实现工具函数，计算批次的损失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5de422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(inputs,targets,model,device):\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(inputs)\n",
    "    loss = F.cross_entropy(outputs.flatten(0,1), targets.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader,model,device,num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader)==0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches,len(data_loader))\n",
    "    for i , (inputs, targets) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        loss = calc_loss_batch(inputs, targets, model, device)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa0768f",
   "metadata": {},
   "source": [
    "### 计算损失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e555299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583266364204\n",
      "Test loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device)\n",
    "print('Training loss:',train_loss)\n",
    "print('Test loss:',test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a22e19",
   "metadata": {},
   "source": [
    "## 5.2 训练大语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c89790ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    #评价模块\n",
    "    model.eval()\n",
    "    #检验模式\n",
    "    with torch.no_grad():\n",
    "        #我认为的双保险,防止梯度更新\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    #\t在评估结束后切换回训练模式，确保模型能继续用于训练。\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b249c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model,train_loader,val_loader,optimizer,device,num_epochs,eval_freq,eval_iter,start_context,tokenizer):\n",
    "    train_losses , val_losses , track_tokens_seen = [],[],[]\n",
    "    tokens_seen , global_step = 0,-1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch , target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen +=input_batch.numel()\n",
    "            global_step +=1\n",
    "            \n",
    "            if global_step % eval_freq ==0:\n",
    "                train_loss , val_loss = evaluate_model(model,train_loader,val_loader,device,eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f'Ep{epoch+1} (Step{global_step:06d}):'\n",
    "                      f'Train loss {train_loss:.3f},'\n",
    "                      f' Val loss {val_loss:.3f},')\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses , val_losses , track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0833ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004,weight_decay=0.1)\n",
    "num_epochs = 10\n",
    "train_losses , val_losses , tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context='Every effort moves you',\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dcb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "#一个经典的plot画图函数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
